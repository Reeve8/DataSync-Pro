{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yb8hhIN4eCC"
      },
      "source": [
        "# Automatic form filling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfmg_Pur4eCP"
      },
      "source": [
        "Required packages:\n",
        "- pip install pypdf2\n",
        "- pip install pymupdf\n",
        "- pip install openai\n",
        "- pip install pandas\n",
        "- pip install openpyxl\n",
        "- pip install numpy\n",
        "- pip install opencv-python\n",
        "- pip install Pillow\n",
        "- pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpZdPpD_4eCQ"
      },
      "source": [
        "*Disclaimer - Considering the API privacy, Please replace api_key and selected_model from the config.py to your custom LLM api key and selected_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yspd35PF4eCR"
      },
      "outputs": [],
      "source": [
        "from config import api_key, selected_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "api key of chat gpt."
      ],
      "metadata": {
        "id": "H-0TXGZ1FdkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boXPfmvE4eCS"
      },
      "outputs": [],
      "source": [
        "# Importing the required Packages\n",
        "import requests\n",
        "import openai\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import os\n",
        "import openpyxl\n",
        "import numpy as np\n",
        "import io\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from math import sqrt\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ6LCqMCKpWh",
        "outputId": "4822241b-802e-4055-f410-ab2517dce1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqki_15DKgPK",
        "outputId": "14580beb-99b6-4a75-a72a-55659a53170a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.2)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.1 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QOzT6jgKXOF",
        "outputId": "89381bd9-a1cc-40af-bf2e-3776f3c1ad49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQl4mXrD4eCS"
      },
      "outputs": [],
      "source": [
        "#extra_questions is the input field that is used to update the unrecognised questions after an initial run\n",
        "extra_questions = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the llm model to detect the files which it wont properly detect, so those questions will be passed in the etra ques, not only the detected questions."
      ],
      "metadata": {
        "id": "wxuGx_T6GFVu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eTPioV84eCT"
      },
      "source": [
        "## The Input resources (should be edited as per the purpose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL_md_Iq4eCT"
      },
      "source": [
        "- Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gREl-U4eCU"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/excell form data.xlsx'#The file which consist of the data that should have to be entered in the form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9iivb2R4eCV"
      },
      "source": [
        "- PDF filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dTPjIeU4eCV"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/BCU-application-form.pdf'#The particular path of the PDF file you want to fill\n",
        "output_pdf_path = '/content/output project .pdf'#the particlar path of a empty PDF file where you want save the filled PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_pBt8Wc4eCV"
      },
      "source": [
        "- Word filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC8pKa9v4eCV"
      },
      "outputs": [],
      "source": [
        "word_path = '/content/authorised_interruption_of_study_application_1.docx'#The particular path of the Word file you want to fill\n",
        "output_path = 'fully filled'#you can give any desired name that you want for the filled form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgoOVUGj4eCV"
      },
      "source": [
        "- Excel Filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQkhuRwV4eCV"
      },
      "outputs": [],
      "source": [
        "excel_path = '/content/StudentAnnualPermitApplicationForm (1).xlsx'#The particular path of the Excel file you want to fill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc_83N4I4eCW"
      },
      "source": [
        "### 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JX3Sene4eCW"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    # Determining the file extension\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "    # checking the file extension to load it as per the extention\n",
        "    if file_extension in ['.xls', '.xlsx']:\n",
        "        # It's an Excel file\n",
        "        df = pd.read_excel(file_path)\n",
        "    elif file_extension in ['.csv', '.txt']:\n",
        "        # It's a CSV or a text file\n",
        "        df = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    # Convert the DataFrame to a dictionary (assuming you want to use all data as context)\n",
        "    data_context_dict = df.to_dict(orient='records')[0]  # Taking the first row as an example\n",
        "\n",
        "    return data_context_dict\n",
        "\n",
        "#Loading the particular data you had given in the input resources\n",
        "data_context_dict = load_data(file_path)\n",
        "# Convert the dictionary to a string context to make it similar to a statement about the particular person with the given data baout him\n",
        "data_context = '. '.join([f\"{key} is {value}\" for key, value in data_context_dict.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5mSdEQB4eCW"
      },
      "source": [
        "### 2. General Functions for Form Filling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91HJXXeo4eCW"
      },
      "outputs": [],
      "source": [
        "#establishing the API connection using the API key and setting the temperature as 0.7\n",
        "def call_chat_gpt(prompt, api_key, selected_model):\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": selected_model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    return response.json()\n",
        "#To extract the content from the generated answers , the function answer_cleanup had been made\n",
        "def answer_cleanup(output):\n",
        "    try:\n",
        "        return output['choices'][0]['message']['content']\n",
        "    except (KeyError, IndexError, TypeError):\n",
        "        return \"Error in processing the response.\"\n",
        "\n",
        "#Now this function is trying to identify the questions from the extracted text of document that is already loaded\n",
        "def identify_keywords_with_llm(pdf_text, api_key, selected_model):\n",
        "#The specific prompt to generate questions\n",
        "    prompt = f\"\"\"\n",
        "Given the text, identify and list only the explicit questions and statements that indicate a requirement for user input. This includes:\n",
        "\n",
        "- Direct questions that clearly ask for specific information (e.g., \"What is your name?\").\n",
        "- Statements or phrases that directly imply a requirement for a response or filling a blank (e.g., \"Name of the applicant: _______\").\n",
        "- Explicit prompts for specific types of information, irrespective of whether they are formatted as questions or statements (e.g., \"Date of birth:\", \"Briefly describe your experience with...\").\n",
        "\n",
        "Exclude:\n",
        "- General descriptive text or narrative explanations.\n",
        "- Any content that does not directly request user input or response.\n",
        "Important formatting requirements that should be implemented for the output:\n",
        "- Remove any leading characters such as dashes ('-'),colon(:), list numbers, or all punctuation from the beginning of each line to focus solely on the content.\n",
        "- Retain the original text format as much as possible beyond the initial character removal to ensure clarity and ease of reading.\n",
        "- Extract the clean text of each question or statement requesting user input, excluding any extraneous elements or formatting characters.\n",
        "- Include all variations of input requests, whether they are formatted as direct questions, implied statements, or instructional texts, without the initial formatting characters.\n",
        "\n",
        "Please process the text with these instructions:\n",
        "\n",
        "{pdf_text}\n",
        "\"\"\"\n",
        "#Calling the gpt with the given prompt\n",
        "    llm_response = call_chat_gpt(prompt, api_key, selected_model)\n",
        "    #doing the answer clean up to extarct the content only\n",
        "    response = answer_cleanup(llm_response)\n",
        "    #spliting the answer from where it has '\\n'(assumming that as an end of a line)\n",
        "    keywords_from_llm_1 = response.split('\\n')\n",
        "    #sometimes the questions generaed might come with specific '-' so here I am striping that from each\n",
        "    keywords_from_llm = [keyword.lstrip('- ').strip() for keyword in keywords_from_llm_1]\n",
        "    return keywords_from_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEPdLhX4eCW"
      },
      "source": [
        "## PDF Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTQX0v2B4eCW"
      },
      "source": [
        "### 3. Functions for PDF Filling Processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSFuy9M64eCW"
      },
      "outputs": [],
      "source": [
        "#Function which is capable of extracting the text from a pdf using fitz library\n",
        "def extract_all_text(pdf_path):\n",
        "    # Open the PDF file\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    all_text = \"\"  # Initialize a variable to store all the extracted text\n",
        "\n",
        "    # Iterate through each page in the PDF\n",
        "    for page in doc:\n",
        "        # Extract text from the current page\n",
        "        page_text = page.get_text()\n",
        "        all_text += page_text + \"\\n\"  # Append the text from this page, add a newline as separator\n",
        "\n",
        "    return all_text\n",
        "#Now generating the answers for the specific questions extracted from the pdf extarcted text\n",
        "def generate_answer_with_context(context, question, api_key, selected_model):\n",
        "    # This is the prompt for answer generation explicitly for PDF extracted text\n",
        "    prompt = prompt = f\"Based on the following details: {context}, if the information is explicitly provided, what is the concise answer (preferably a single word or a short phrase) for '{question}'? Keep the response brief. If not enough information is available for a question, just respond with ''.\"\n",
        "    #calling my gpt to generate the answers with the prompt I had given\n",
        "    response = call_chat_gpt(prompt, api_key, selected_model)\n",
        "    cleaned_response = answer_cleanup(response).strip()\n",
        "#In PDF forms only I am replacing the insufficient data with an empty string(\"\")\n",
        "    if cleaned_response.lower() == \"\":\n",
        "        return None\n",
        "    else:\n",
        "        return cleaned_response\n",
        "\n",
        "#Now I am creating function specifically for the box detection in the pdf forms\n",
        "#This function is converting my pdf pages into images by loading each of the pages\n",
        "def extract_images_from_pdf(pdf_path):\n",
        "    images = []\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page in doc:\n",
        "        pix = page.get_pixmap()\n",
        "        img_bytes = pix.tobytes(output=\"png\")\n",
        "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "        image_np = np.array(image)\n",
        "        # Convert RGB to BGR for OpenCV\n",
        "        image_np = image_np[:, :, ::-1].copy()\n",
        "        images.append(image_np)\n",
        "    return images\n",
        "\n",
        "#Now I am doing the greyscale conversion, guassian blur, edge detection and finding the contours to find the coordinates of the boxes in the form\n",
        "def find_boxes_in_image(image):\n",
        "    boxes = []\n",
        "    try:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "        edges = cv2.Canny(blur, 100, 200)\n",
        "        contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        for contour in contours:\n",
        "            approx = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n",
        "            if len(approx) == 4:\n",
        "                # approx contains the coordinates of the 4 corners\n",
        "                corners = approx.reshape(-1, 2)\n",
        "                boxes.append(corners.tolist())\n",
        "    except cv2.error as e:\n",
        "        print(f\"Error in finding boxes: {e}\")\n",
        "    return boxes\n",
        "\n",
        "#my previous function was detecting some unnneccesary small boxes along with actual boxes\n",
        "#So here I am calculating the area of all the boxes\n",
        "def calculate_rectangle_area(box):\n",
        "    xs = [point[0] for point in box]\n",
        "    ys = [point[1] for point in box]\n",
        "    width = max(xs) - min(xs)\n",
        "    height = max(ys) - min(ys)\n",
        "    return width * height\n",
        "\n",
        "#The next step is to find the coordinates of the identifird questions by LLM\n",
        "def extract_text_coordinates(pdf_path, keywords):\n",
        "\n",
        "    doc = fitz.open(pdf_path)  # Open the PDF\n",
        "    text_info = []  # List to hold text and coordinates\n",
        "\n",
        "    for page_num in range(len(doc)):  # Iterate through each page\n",
        "        page = doc.load_page(page_num)  # Load the current page\n",
        "        text_instances = page.get_text(\"dict\")[\"blocks\"]  # Extract text instances as dictionaries\n",
        "\n",
        "        for instance in text_instances:\n",
        "            if 'lines' in instance:  # Check if the block contains lines of text\n",
        "                for line in instance['lines']:\n",
        "                    for span in line['spans']:  # Iterate through each text span in the line\n",
        "                        text = span['text']\n",
        "                        if any(keyword.lower() in text.lower() for keyword in keywords):\n",
        "                            bbox = span['bbox']  # Bounding box of the text\n",
        "\n",
        "                            # Calculate all four corners based on bbox\n",
        "                            corners = [\n",
        "                                (bbox[0], bbox[1]),  # Bottom-left\n",
        "                                (bbox[2], bbox[1]),  # Bottom-right\n",
        "                                (bbox[2], bbox[3]),  # Top-right\n",
        "                                (bbox[0], bbox[3])   # Top-left\n",
        "                            ]\n",
        "                            text_info.append({\n",
        "                                \"page\": page_num,\n",
        "                                \"text\": text,\n",
        "                                \"corners\": corners  # List of all four corner coordinates\n",
        "                            })\n",
        "\n",
        "    return text_info\n",
        "\n",
        "\n",
        "#Now using the universal mathematical concept euclidean distance to find the distance between each question and all the answer boxes to find the shortest one\n",
        "\n",
        "def calculate_euclidean_distance(point1, point2):\n",
        "\n",
        "    return sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
        "#Using the below function I am assigning the closest box which means the one box where the answers should go for each question by calculating the eucledean distance as mentioned above. And assigning the short distanced one as the corresponding box\n",
        "\n",
        "def find_closest_box(text_coordinates, box_coordinates):\n",
        "    closest_boxes = []\n",
        "\n",
        "    for text_item in text_coordinates:\n",
        "        question_page = text_item['page']  # Get the page number of the text item\n",
        "        # Assuming the corners are ordered as bottom-left, bottom-right, top-right, top-left\n",
        "        text_top_right = text_item['corners'][2]  # Top right corner of the text\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        closest_box = None  # Initialize closest box to None\n",
        "\n",
        "        # Ensure we're looking at the correct page's boxes\n",
        "        if question_page < len(box_coordinates):\n",
        "            for box in box_coordinates[question_page]:\n",
        "                # Correctly access the top left corner of the box directly\n",
        "                # Assuming corners order: bottom-left, bottom-right, top-right, top-left\n",
        "                box_top_left = box[3]  # Direct access to the top left corner\n",
        "\n",
        "                # Calculate Euclidean distance from text top right to box top left because we want to calculate the most closest portion between a question and its corresponding box\n",
        "                distance = calculate_euclidean_distance(text_top_right, box_top_left)\n",
        "\n",
        "                # Update the closest box if the current box is closer\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_box = box\n",
        "\n",
        "        # Append the result for the current question to the list\n",
        "        closest_boxes.append({\n",
        "            'question': text_item['text'],\n",
        "            'closest_box': closest_box,\n",
        "            'page': question_page\n",
        "        })\n",
        "\n",
        "    return closest_boxes\n",
        "\n",
        "#Now I want to print the generated answers into the pdf and want to save it into the output file path\n",
        "def save_generated_pdf(input_pdf_path, question_answer_boxes, answers, output_pdf_path, closest_boxes):\n",
        "    doc = fitz.open(input_pdf_path)\n",
        "\n",
        "    for item in closest_boxes:\n",
        "        question = item['question'].strip(':')  # Remove colon for matching\n",
        "        closest_box = item['closest_box']\n",
        "        page_number = item['page']\n",
        "        answer = answers.get(question, '')  # Get the answer for the question\n",
        "\n",
        "        if answer:\n",
        "            # Assuming closest_box is a list of points [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
        "            # and we'll use the top-left corner to position the text\n",
        "            # Sorting by x to get left points, then by y to get the topmost point\n",
        "            top_left_corner = sorted(closest_box, key=lambda point: (point[0], point[1]))[0]\n",
        "            # Adjusting the position slightly to the right (x + 2) and down (y + 10) for better visibility\n",
        "            position = (top_left_corner[0] + 2, top_left_corner[1] + 10)\n",
        "\n",
        "            page = doc.load_page(page_number)\n",
        "            # Insert the text at the adjusted position\n",
        "            page.insert_text(position, answer, fontsize=10)\n",
        "#lastly saving the file to the output file path\n",
        "    doc.save(output_pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzAvJglW4eCX"
      },
      "source": [
        "### 4. Implementing the functions to fill the PDF form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpwOotiq4eCX"
      },
      "outputs": [],
      "source": [
        "# Extract all text from the PDF\n",
        "extracted_text = extract_all_text(pdf_path)\n",
        "\n",
        "\n",
        "# Call the LLM with the extracted PDF text to identify the questions\n",
        "keywords_from_llm = identify_keywords_with_llm(extracted_text, api_key, selected_model)\n",
        "\n",
        "# Including the extra question if there is anything in that list\n",
        "if extra_questions:\n",
        "    keywords_from_llm.extend(extra_questions)\n",
        "\n",
        "# Generating answers for each keyword\n",
        "answers = {keyword: generate_answer_with_context(data_context_dict, keyword, api_key, selected_model) for keyword in keywords_from_llm}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " keywords_from_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VAAOYxta8GH",
        "outputId": "fb611ceb-b7a2-4557-d549-67f87e46a036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Course Title:',\n",
              " 'Proposed start date:',\n",
              " 'Proposed Year/Level of Entry:',\n",
              " 'Date of birth:',\n",
              " 'First name(s):',\n",
              " 'Maiden or any other name(s) that you have been known by:',\n",
              " 'Surname/family name:',\n",
              " 'Permanent address:',\n",
              " 'Correspondence address (if different):',\n",
              " 'Daytime telephone:',\n",
              " 'Evening telephone (if different):',\n",
              " 'Mobile:',\n",
              " 'Email address:',\n",
              " 'Nationality:',\n",
              " 'If not born in the UK please state date of arrival to UK:',\n",
              " 'Area of permanent residence:',\n",
              " 'If you are a member of a Professional Body, please give its name and your Registration Number:',\n",
              " 'Have you ever studied in the UK before? (If yes, please include a copy of all visas)',\n",
              " 'What level was your previous study in the UK (please tick all that apply)?',\n",
              " 'Have you ever studied at Birmingham City University before?',\n",
              " 'Please enter details of the highest level of qualification you currently hold.',\n",
              " 'If you have a 10 digit Unique Learner Number (ULN), please enter it in the box provided.',\n",
              " 'If you are an overseas student please include your IELTS/TOEFL results below:',\n",
              " 'Please give details of work experience, training and employment in reverse chronological order.',\n",
              " 'Please enter here any further information in support of your application, for example, reasons for choosing the course, your professional career to date (if relevant) and your current career goals.',\n",
              " 'Did you use an agent to help you find this course? Yes No',\n",
              " 'PLEASE INDICATE HOW YOU HEARD ABOUT THE COURSE (please tick relevant boxes):',\n",
              " 'Do you have any special needs? (please tick).',\n",
              " 'Applicant’s name:',\n",
              " 'Applicant’s signature:',\n",
              " 'Date:']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DyU9at4eCX"
      },
      "source": [
        "#### 4.1 Using Image Analysis in box detection and filling the form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BC_jTHo4eCX"
      },
      "outputs": [],
      "source": [
        "# Extracting the images from PDF\n",
        "images = extract_images_from_pdf(pdf_path)\n",
        "\n",
        "# Initialize an empty list to hold the box coordinates for all images\n",
        "box_coordinates = []\n",
        "\n",
        "# Find answer box coordinates for each image and add them to the box_coordinates list\n",
        "for i, image in enumerate(images):\n",
        "    boxes = find_boxes_in_image(image)\n",
        "    box_coordinates.append(boxes)   # Add the boxes for this image to the main list\n",
        "\n",
        "filtered_box_coordinates = []#the boxes to append the filtered boxes as per the area\n",
        "\n",
        "for page_boxes in box_coordinates:\n",
        "    # Filter boxes based on their calculated area by assigning 50 as a threshold area\n",
        "    filtered_boxes = [box for box in page_boxes if calculate_rectangle_area(box) >= 50]\n",
        "    filtered_box_coordinates.append(filtered_boxes)\n",
        "\n",
        "#Extracting the coordinates of the questions identified by the LLMs\n",
        "text_coordinates = extract_text_coordinates(pdf_path,keywords_from_llm)\n",
        "#Finding the closest boxes by calculating the euclidean distances and assigning the short distanced box as the corresponding box\n",
        "closest_boxes = find_closest_box(text_coordinates, filtered_box_coordinates)\n",
        "#inserting the answers in the corresponding boxes and saving the file\n",
        "save_generated_pdf(pdf_path, closest_boxes, answers, output_pdf_path, closest_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqrM89UU4eCX"
      },
      "source": [
        "##  Word Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VUoMvQc4eCX"
      },
      "source": [
        "### 5. Functions for Word Filling Processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxFpMRqV4eCY"
      },
      "outputs": [],
      "source": [
        "#so in here the word files might be in either .docx format or .doc format this function convert the doc to docx to make it work on any formats and making it as much as flexible\n",
        "def convert_doc_to_docx(doc_path):\n",
        "    # Ensure the path is absolute\n",
        "    doc_path = os.path.abspath(doc_path)\n",
        "    # Create a .docx path for the converted file\n",
        "    docx_path = os.path.splitext(doc_path)[0] + \".docx\"\n",
        "\n",
        "    # Start Word application\n",
        "    word = win32.gencache.EnsureDispatch('Word.Application')\n",
        "    doc = word.Documents.Open(doc_path)\n",
        "    # Save the document in .docx format\n",
        "    doc.SaveAs2(docx_path, FileFormat=16)  # 16 represents wdFormatXMLDocument\n",
        "    doc.Close()\n",
        "    word.Quit()\n",
        "\n",
        "    return docx_path\n",
        "#So now I am extracting the texts of the word file especially from parigrph and tables\n",
        "def extract_text_in_order(file_path):\n",
        "    # Check if file is .doc, convert to .docx\n",
        "    if file_path.endswith('.doc'):\n",
        "        print(\"Converting .doc to .docx\")\n",
        "        file_path = convert_doc_to_docx(file_path)\n",
        "\n",
        "    doc = Document(file_path)\n",
        "    extracted_text = []\n",
        "\n",
        "    # Extract text from paragraphs\n",
        "    for para in doc.paragraphs:\n",
        "        extracted_text.append(para.text)\n",
        "\n",
        "    # Extract text from tables\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            for cell in row.cells:\n",
        "                extracted_text.append(cell.text)\n",
        "\n",
        "    return extracted_text\n",
        "#So this function is precisely for generating answers for the questions generated from the word extracted text\n",
        "def generate_answer_with_context(context, question, api_key, selected_model):\n",
        "    # Update the prompt to specify that the response should be \"Insufficient data\" if not enough context is provided\n",
        "    prompt = (f\"Based on the following details: {context}, for the question '{question}', \"\n",
        "                  \"what is the concise answer (preferably a single word or a short phrase)? \"\n",
        "                  \"If not enough information is available, respond with 'Insufficient data'. \"\n",
        "                  \"For imaginative or creative questions, where crafting a story or providing an imaginative answer is suitable,and do not condider the condition ogf making the answer concise while you create a story for a question \"\n",
        "                  \"use the available context to craft a compelling, creative response that aligns with the theme or nature of the question.\")\n",
        "# Use this 'prompt' with your LLM to generate the response.\n",
        "\n",
        "    response = call_chat_gpt(prompt, api_key, selected_model)\n",
        "    cleaned_response = answer_cleanup(response).strip()\n",
        "\n",
        "    # Check if the response is \"Insufficient data\" or similar to indicate not to generate an answer\n",
        "    if cleaned_response.lower() == \"\":\n",
        "        return None  # Or any suitable indicator for no answer due to insufficient context\n",
        "    else:\n",
        "        return cleaned_response\n",
        "\n",
        "#So this function is helping as to manupulate the word files ,initially by converting the doc into docx because the docx library can only do manupulation if the document is in . docx format\n",
        "def fill_in_answers(doc_path, answers_dict, output_file_name):\n",
        "    # Check if the file is a .doc file, and convert it to .docx if so\n",
        "    if doc_path.endswith(\".doc\"):\n",
        "        print(\"Converting .doc to .docx\")\n",
        "        doc_path = convert_doc_to_docx(doc_path)\n",
        "\n",
        "    doc = Document(doc_path)\n",
        "\n",
        "    # Process paragraphs in the document body\n",
        "    for i, para in enumerate(doc.paragraphs):\n",
        "        for keyword, answer in answers_dict.items():\n",
        "            if keyword.lower() in para.text.lower():  # Case insensitive search for the keyword\n",
        "                # Check if the next paragraph is empty and we are not at the last paragraph\n",
        "                if i + 1 < len(doc.paragraphs) and not doc.paragraphs[i + 1].text.strip():\n",
        "                    # Insert the answer into the next paragraph\n",
        "                    doc.paragraphs[i + 1].text = answer\n",
        "\n",
        "    # Process each table in the document\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            for i, cell in enumerate(row.cells):\n",
        "                for keyword, answer in answers_dict.items():\n",
        "                    if keyword.lower() in cell.text.lower():\n",
        "                        # Attempt to find the next empty cell in the row\n",
        "                        if i + 1 < len(row.cells) and not row.cells[i + 1].text.strip():\n",
        "                            row.cells[i + 1].text = answer\n",
        "\n",
        "    # Construct the output path with .docx extension\n",
        "    output_dir = os.path.dirname(doc_path)  # Use the same directory as the input file\n",
        "    output_path = os.path.join(output_dir, output_file_name + \".docx\")\n",
        "\n",
        "    # Save the document to the specified output path\n",
        "    doc.save(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDGO4Oc4eCY"
      },
      "source": [
        "### 6. Implementing the functions to fill the word form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbri3zCJ4eCY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extracting all texts from the word file\n",
        "extracted_text = extract_text_in_order(word_path)\n",
        "\n",
        "\n",
        "# Calling the gpt to identify the question from the extracted text\n",
        "keywords_from_llm = identify_keywords_with_llm(extracted_text, api_key, selected_model)\n",
        "\n",
        "# Including the extra question if there is anything in that list\n",
        "if extra_questions:\n",
        "    keywords_from_llm.extend(extra_questions)\n",
        "\n",
        "# Generate answers for each keyword\n",
        "answers = {keyword: generate_answer_with_context(data_context_dict, keyword, api_key, selected_model) for keyword in keywords_from_llm}\n",
        "#Filling the answers into the next empty space of each identified question\n",
        "fill_in_answers(word_path,answers,output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTDmeD_D4eCY"
      },
      "source": [
        "## Excel Spreadsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov-Of8yF4eCY"
      },
      "source": [
        "### 7. Functions for Excel filling processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k0mV7B04eCY"
      },
      "outputs": [],
      "source": [
        "#The function specifically for generating answers for the questions extracted from excel file\n",
        "def generate_answer_with_context(context, question, api_key, selected_model):\n",
        "    # Update the prompt to specify that the response should be \"Insufficient data\" if not enough context is provided\n",
        "    prompt = f\"Based on the following details: {context}, answer the question '{question}' as follows: If the question is a direct query seeking specific information (e.g., 'What is the capital of France?'), provide a concise answer (preferably a single word or a short phrase). If the question is analytical and seeks a deeper explanation (e.g., 'How does photosynthesis work?'), provide a detailed and analytical answer. For yes/no questions (e.g., 'Is the sky blue?'), respond with 'Yes' or 'No' and include a brief justification. If not enough information is available to accurately answer the question, respond with 'Insufficient information.'and only provide answers for a question once and in the output I just need the answers to be alone not with the questions\"\n",
        "\n",
        "\n",
        "#calling the gpt to generate answers foe each question\n",
        "    response = call_chat_gpt(prompt, api_key, selected_model)\n",
        "    cleaned_response = answer_cleanup(response).strip()\n",
        "\n",
        "    # Check if the response is \"Insufficient data\" or similar to indicate not to generate an answer\n",
        "    if cleaned_response.lower() == \"\":\n",
        "        return None  # Or any suitable indicator for no answer due to insufficient context\n",
        "    else:\n",
        "        return cleaned_response\n",
        "    #some times the files might be in csv format\n",
        "    # Function to convert CSV to XLSX if necessary and return the path to the XLSX file\n",
        "def ensure_xlsx(file_path):\n",
        "    dir_name, file_name = os.path.split(file_path)\n",
        "    base_name, extension = os.path.splitext(file_name)\n",
        "\n",
        "    # If the file is a CSV, convert it to XLSX\n",
        "    if extension.lower() == '.csv':\n",
        "        df = pd.read_csv(file_path)\n",
        "        new_file_path = os.path.join(dir_name, base_name + '.xlsx')\n",
        "        df.to_excel(new_file_path, index=False)  # Save the DataFrame as an XLSX file\n",
        "        return new_file_path\n",
        "    elif extension.lower() in ['.xls', '.xlsx']:\n",
        "        return file_path\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbYa4f1e4eCY"
      },
      "source": [
        "### 8. Implementing the functions to fill the Excel form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2gjNVbY4eCZ"
      },
      "outputs": [],
      "source": [
        "#loading the data using pandas\n",
        "data = pd.read_excel(excel_path)\n",
        "#converting the loaded data into string values\n",
        "data_string = '\\n'.join(data.applymap(str).stack().tolist())\n",
        "#calling gpt to identify the questions from the data string\n",
        "keywords_from_llm = identify_keywords_with_llm(data_string, api_key, selected_model)\n",
        "\n",
        "# Including the extra question if there is anything in that list\n",
        "if extra_questions:\n",
        "    keywords_from_llm.extend(extra_questions)\n",
        "\n",
        "# Generate answers for each keyword\n",
        "answers = {keyword: generate_answer_with_context(data_context_dict, keyword, api_key, selected_model) for keyword in keywords_from_llm}\n",
        "# Ensure the file is in XLSX format\n",
        "xlsx_file_path = ensure_xlsx(excel_path)\n",
        "#From here onwards using the openpyxl library I am trying to fill the answers to the corresponding empty cells to each questions\n",
        "wb = openpyxl.load_workbook(xlsx_file_path)\n",
        "sheet = wb.active\n",
        "\n",
        "# Iterate over the rows and columns to find the questions\n",
        "for row in range(1, sheet.max_row + 1):\n",
        "    for col in range(1, sheet.max_column + 1):\n",
        "        cell = sheet.cell(row=row, column=col)\n",
        "        cell_value = cell.value\n",
        "        if cell_value in answers:\n",
        "            # Once the question is found, search for the next empty cell in the same row\n",
        "            for answer_col in range(col + 1, sheet.max_column + 2):\n",
        "                next_cell = sheet.cell(row=row, column=answer_col)\n",
        "                if next_cell.value is None or isinstance(next_cell, openpyxl.cell.cell.MergedCell):\n",
        "                    # Find the top-left cell of the merged range that includes the next_cell\n",
        "                    for range_ in sheet.merged_cells.ranges:\n",
        "                        if next_cell.coordinate in range_:\n",
        "                            top_left_cell = sheet.cell(range_.min_row, range_.min_col)\n",
        "                            top_left_cell.value = answers[cell_value]\n",
        "                            break\n",
        "                    else:\n",
        "                        # If next_cell is not a merged cell, just assign the value directly\n",
        "                        next_cell.value = answers[cell_value]\n",
        "                    break  # Exit the loop after inserting the answer\n",
        "\n",
        "# Define a new file name by appending \"filled\" before the file extension in the original file path\n",
        "dir_name, file_name = os.path.split(excel_path)\n",
        "base_name, extension = os.path.splitext(file_name)\n",
        "new_file_name = base_name + \"_filled\" + extension\n",
        "new_file_path = os.path.join(dir_name, new_file_name)\n",
        "\n",
        "# Save the updated workbook to the new file path\n",
        "wb.save(new_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNBac8IX4eCZ"
      },
      "source": [
        "## Appendix\n",
        "*Disclaimer - In the supplementary materials, I've included two separate folders: one contains all the forms I've filled, and the other holds the original formats of these forms. For demonstration purposes, I've provided a single completed form in each of the three formats. This arrangement will give you access to both the finalized forms and their initial templates."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}